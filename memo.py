#チートシート
#検索して使う用

#===================================================
#手順
#===================================================

# 1.タスクと評価指標
# 2.特徴量の作成
# 3.モデルの作成
# 4.モデルの評価
# 5.モデルのチューニング
# 6.アンサンブル

#===================================================
# 1.タスクと評価指標
#===================================================

#まずは問題の理解
#特徴量と目的変数に分ける
#統計量を見る or 可視化手法　を見てデータを把握する。

#次に評価指標と目的関数を決定する。
#評価指標
#回帰ー＞RMSE・RMSLE・MAE・決定係数
#二値分類(予測値が正・負)ー＞confusion matrix・accuracy/error rate・precision/recall・F1score/Fβscore・MCC
#二値分類(予測値が正)ー＞logloss・AUC
#多クラス分類ー＞multi-class accuracy・multi-class logloss・mean-F1/macro-F1/micro-F1・quadratic weighted kappa
#レコメンデーションー＞MAP@K

#目標関数
#回帰タスクー＞RMASE
#分類タスクー＞logloss
#評価指標とモデルの学習で使用する目的関数が一致していれば分かりやすい！！ー＞カスタムするのがよい

#評価指標の最適化
# 1.正しくモデリングをおこなう。
# 2.学習データの前処理、異なる評価指標を使う。
# 3.異なる評価指標の最適化で、後処理をおこなう。
# 4.カスタム目的関数を使う。
# 5.異なる評価指標を最適化、アーリートッピングをする。


#閾値の最適化
#迷ったらこれ！ー＞Nelder-Mead/COBYLA
#複雑な最適化時ー＞out-of-fold


#予測確率とその調整
#分類タスクに置いて評価指標の最適化をするために、妥当な予測確率を必要とする。loglossだと予測確率がずれているとスコアが下がる。
#GBDT、ニューラルネットワーク、ロジスティック回帰などのモデルはloglossを目的関数とするため、妥当
#しかし、データが十分じゃない時と、ランダムフォレストを使う時はloglossではないので注意

#調整には、予測値をn乗する。出力範囲を絞る。スタッキングする。CalibratedClassifierCVを使う。

#===================================================
# 2.特徴量
#===================================================

#変数の変換により特徴量を決める->決定木の気持ちになって考える。相互作用を直接的に表現した特徴量がgood!

#欠損値->GBDTを使う場合には、1)欠損値をそのまま扱う。2)欠損値埋めるほうが良い精度になる可能性がある。もしくは3)欠損値から特徴量を作るのもあり。
#1)->そのまま使う。通常取りうる値の範囲外を代入する。
#2)->平均値を代入して埋める。or 中央値など。
#3)->欠損値を予測変換する。欠損値のないデータから欠損値を作成するモデルを作る。欠損していること自体に意味を見出す。便利なのは、
#pd.read_csv('train.csv', na_values=['', 'NA', -1, 9999])ってしたら、欠損値だけを読み取れる。
#欠損値を変換するのに、data['col1'] = data['col1'].replace(-1, np.nan)

#数値変換->1)標準化、2)min-maxスケーリング、3)非線形変換、4)clipping、5)binning、6)順位への変換、7)RankGauss
#　1)線形変換して、変数の平均を0、標準偏差を1とする標準化をする。線形回帰やNNでおすすめ。
#　2)画像データなら最小値と最大値決まっているからgood
#　3)非線形変換は、変数の分布の形状を変えないので、1)2)に比べて優位。
    #log1p(x)関数
    #Box-Cox変換(対数変換)、Yeo-johnson変換(負の値を持つ変数にも適用可能)
    #generalizeed transformaton
    #絶対値
    #平方根
    #二乗する、n乗する
    #２値変換
    #数値の端数をとる
    #四捨五入。切り上げ。切り捨て
#　4)外れ値が含まれる場合に、上限や下限から外れたならばその数値を置き換えて無視。
#　5)数値変換を区間ごとにグループ分けして、あえてカテゴリ変数として扱う。
#　6)数値変換を大小関係に基づいた順位へと変換する方法。
#　7)順位を保ったまま半ば無理やり正規分布になるようにする変換手法。NNにおすすめ
    #やり方は、QuantileTransformerくらすで、n_quantilesを十分大きくしてoutput_distribution='normal'とする。

#カテゴリ変数変換->1)one-hot encoding、2)label encoding、3)feature hashing、4)frequency encoding、5)target encoding、6)embidding、7)順序変数の扱い、8)カテゴリ変数の値の意味を抽出する
#モデルごとに適した形に変換する必要がある。文字列・数値の大きさに意味のない場合。カテゴリ変数化すべき
    #　1)get_dummies関数か、preprocessingモジュールを使う。
    #　2)GBDTにおいては、label encodingはカテゴリ変数を変換する基本的な方法。単純に整数化するだけ。
    #　3)ハッシュ関数による計算によって異なる水準でも同じ場所にフラグを立てる感じ。
    #　4)出現回数でカテゴリ変数を置き換える方法。
    #　5)目的変数を用いてカテゴリ変数を数値に変換する。時間列性には弱いが、非常に役に立つ方法。
        #　注意！！！！！！！！！
        #　単純にデータ全体から平均を取ると、自身のレコードの目的変数をカテゴリ変数にとりこんでしまうので、リークしてしまう。自身のレコードの目的変数を使わないように変換することが必要
        #  目的変数の平均のとり方。回帰->目的変数の平均。二値分類->正例1、負例0として平均取る。多クラス分類->クラスの数だけ二値分類してクラスの数だけtarget encodingによる特徴量作成
    #　6)単語やカテゴリ変数のような離散的な保湯源を、実数ベクトルに変換する手法
    #　7)値の順序のみに意味がある。その値の間隔には意味がない。
    #　8)カテゴリ変数の一部だけが意味を持つ時に分離させるということ。
#日付・時刻を表す変数変換->1)学修データとテストデータの分割。周期的な動きを捉えるための充分なデータか。周期性をもつ変数の扱い。2)日付・時刻を表す変数変換による特徴量
    #　現実にそった考慮が必要。この年は。。。。など
    #　時間をわかりやすく計算してみる。
    #　時間差まで検討する。
#変数の組み合わせ->1)数値変数×カテゴリ変数、2)数値変数×数値変数、3)カテゴリ変数×カテゴリ変数、4)行の統計量を取る
    #　1)数値変数の平均や分散といった統計量をとることで、新たな特徴量を作る
    #　2)加減乗算することで、新たな特徴量を作る
    #　3)複数のカテゴリ変数を組み合わせる。target encodingが有効。onehotだと多すぎ。label encodingなら変数同士を組み合わせる意味がない。
    #　4)意味を考えて、欠損値、ゼロなどをカウントする。
#別のテーブルデータや時系列データから特徴量を作成->1)単純な統計量を取る、2)時間的な統計量を取る。3)条件を絞る。4)集約する単位を変える。5)ユーザーではなくアイテムに注目する。
    #　1)カウント数・ユニーク数のカウント・存在するかどうか・合計や平均や割合・最大や最小標準偏差など。
    #　2)頻度など。出来事の順番など。間隔
    #　3)お昼の時間帯など。特定のwebページなど。条件を絞る。
    #　4)同じ性別や年齢層など。同じモノをキーワードに見つける。
    #　5)アイテムやイベントに注目してみると。性質を捉えやすい
        #アイテム側のグループ化
        #特殊な商品に注目する
        #アイテム側に注目した特徴量の作り方
#時系列データのあつかい->1)時間情報を持つ変数、2)予測する時点よりも過去の情報のみを使う。3)ワイドフォーマット・ロングフォーマット、4)ラグ特徴量がとれる形式かどうか
    #　1)の場合
        #　ユーザの属性や過去の行動ログが与えられる
        # 予測対象は1ヶ月以内に解約するかどうか
        # ある時点のユーザを分割して、学習データ・テストデータが作成されているか
    #　2)の場合
        # 将来のデータ使ったらその少し前の将来のデータを求めるのは簡単。
    #　3)の場合
        # ワイドフォーマットは色んな情報がある横に広い->Dataframeのstackメソッドでワイドからロングに変換可
        # ロングフォーマットは縦に長い。->Dataframeのpivotメソッドでロングからワイドにできる。
    #  4)の場合
        # shift関数をつかうと、時期をずらした値を取得できる。
        #移動平均もおすすめ。rolling関数&mean or rolling関数&max
#次元削減・教師なし学習による特徴量->1)主成分分析;PCA、2)非負値行列因子分解、3)Latent Dirichlet Allocation;LDA、4)線形判別分析;LDA、5)t-SNE、UMAP、6)オートエンコーダ、7)クラスタリング
    # 1)多次元データを分散の大きい方向から順に軸を取り出す方法。変数間の従属性が大きい場合により少数の主成分でもとのデータを表現する。扱うデータは正規分布に従っていればgood
    # 2)非負の行列データをより少数の要素の非負の行列積で近似する方法。
    # 3)確率的生成モデルの一種。ベクトルの要素に分類して、各トピックに各単語がどれぐらい出現するかを計算する。
    # 4)学習データを上手く分類できるような低次元の特徴空間を探す。もとの特徴量をその空間に射影することで次元を削減する。
    # 5)2次元平面上に圧縮して可視化目的に使われる。非線形な関係を捉えることができて精度が上がることがある。計算コスト大きいから2次元や3次元は不向き。UMAPの方がはやいし、クラス分け断然よい。
    # 6)ニューラルネットワークを用いた次元圧縮の方法。入力次元より小さい中間層を用いて、入力と同じ値の出力をおこなうNNを学修して、もとのデータよりも低次元の表現をする。
    # 7)教師なし学習で、クラスタ中心からの距離などを特徴量とする。
#その他->
# 1)背景にあるメカニズムを考える。2)レコードの関係性について注目する。3)相対値に着目。4)位置情報に着目、
# 5)自然言語処理の手法の応用、6)トピックモデルの応用によるカテゴリ変数の変換、7)画像特徴量を扱う手法、
# 8)decision tree featureっｂ transformation、9)匿名化されたデータの変換前の数値を予測。10)データのあやまりを訂正してみる
        # ユーザーの行動について考える。
        # サービスを提供する側の動きについて考える。
        # 業界ではどんなふうに考えているのか。
        # 複数の変数を組み合わせて指数を作る。
        # 自然現象のメカニズムを考える。
        # 分析コンペの対象となっているサービスを実際に試してみる。

#===================================================
# 3.モデルの作成
#===================================================

#1.モデルの種類とハイパーパラメータを指定
#2.学習データ・目的変数を与えて学習させる
#3.テストデータを与えて、予測させる

#1.
#params = {'param1': 10, 'param2': 100}

#2.
#model = Model(params)
#model.fit(train_x, train_y)
#3.
#pred = model.predict(test_x)

#クロスバリデ-ションをしろ。



#特徴量生成が5割から8割
#ハイパーパラメータは時折。本格的な調整は最後
#最初はGBDTモデル。タスクの性質によってNNなど。
#データやタスクの理解が勧めば、バリデーションの枠組みを考える。


#分析コンベで使われるモデル
    #GBDT
    #NN
    #線形モデル
    #その他モデル
        #判断基準は、精度・計算速度・使いやすさ・アンサンブルの多様性によって精度向上に寄与するか

#過去のソリューションを改良する形でどんどんモデルを構成していく。

#===================================================
# 4.モデルの評価
#===================================================

#モデルの評価方法のために、バリデーションデータへの予測の精度を評価する。

#バリデーションの種類
    #hold-out法　
        #一部だけをバリデーション用に置いておく。
        #scikit-learnのmodel_selectionのtrain_test_splitを使えば良い。
        #念のためにシャッフルすることも忘れずに行なう。
    #クロスバリデーション
        #CVと略されることもある。分割して順番にバリデーションデータとして取り扱う。
        #計算時間と学習データの割合とバリデーションスコアを見て判断する。トレードオフの関係にある。
    #stratified k-fold
        #分類タスクの場合に、fold毎に含まれるクラスの割合を等しくする。これを層化抽出という。
        #テストデータに含まれる各クラスの割合は、学習データに含まれる各クラスの割合とほぼ同じという過程に基づくもの。
        #データを分割した時に、各クラスの割合にムラが生じないようにするためのもの。
    #group k-fold
        #顧客が頻繁に出てくると、そのモデルは、その顧客専用のものになってしまう。
        #だから、それを中心に分割をおこなう。
    #leave-one-out
        #学習データのレコード数が極めて少ない場合
        #fold数を学習データのレコード数と同等にして、バリデーションデータを1件だけにする。
        #対処法は、各foldでアーリーストッピングをおこない、その平均などで適切なイテレーション数を見積もったあと、そのイテレーション数を固定して再度クロスバリデーションする

#時系列データのバリデーション手法を説明する
    #時系列のhold-out法
        #時系列のデータは時間的に近いほどデータの傾向が似ていることがあるので注意
        #周期性のあるデータは、1年前のデータを使うなどしたほうがよい。
    #時系列データのクロスバリデーション
        #folf毎に学習データの長さを変える。
            #学習データの期間を最初からとする場合
            #学習データの期間の長さを揃える場合
            #TimeSeriesSpilitがあるが、データの並び順だけなので使える場面は限定的。


#バリデーションのポイントとテクニック
    #バリデーションをおこなう目的
        #モデル改善の指針となるスコアを示すため
        #テストデータに対するスコアやそのばらつきを見積もるため

    #どのようにバリデーションデータを作る?
        #学習データとテストデータの分割を真似る。
        #ランダムにしてしまうとフェアではない結果になってしまうかもしれないので要注意
        #ポイントは、評価に使ったデータと学習データの関係が、本番の予測対象のデータと取得可能な学習データの関係になっているか。
        #学習データとテストデータの分布が違う場合には、adversarial validationがおすすめ。
            #adversarial validationとは。学習データとテストデータを結合して、テストデータか否かを目的変数とする２値分類を行ない、分布を判断する方法。
                #AUCが0.5なら同じということで、ok AUCが1に近くなったら確実に見分けられる方法があるということがわかる。
        #バリデーションとleaderboardのスコアの差異を考察する。
            #偶然によるもの
            #バリデーションとテストデータの分布が異なる場合
            #バリデーションの設計が不適切で、汎化性能を正しく評価できていない。
            #各提出における手元でのバリデーションスコアとpublic Leaderboardのスコアをプロットすることがおすすめ

#===================================================
# 5.モデルのチューニング
#===================================================

#パラメータのチューニング・特徴選択および特徴量の重要度・クラスの分布が偏っている場合

# パラメータのチューニング
    #手動・グリッドサーチ/ランダムサーチ・ベイズ最適化
        #手動は、頑張れば良い成果が出せるかも。その人次第。
        #グリッドサーチ/ランダムサーチは、ランダムサーチのほうが、重要なパラメ-タにたいして反応できる可能性が高いので効率的。
        #以前に計算したパラメータの履歴に基づいた探索スべきパラメータの確率の枠組みを考えるもの。

    #総じて、おすすめは、
    #　1.ベースラインとなるパラメータで学習
        #過去のコンペから探し出す。
    #　2.簡単な調節は、1-3種類のパラメータとそれぞれ2-5個ぐらいの候補でグリッドサーチする。
        #わかりやすさ->グリッドサーチ。精度なら->ベイズ最適化。よって　この方法では、ランダムサーチを使う場面が見当たらない。
        #パラメータの範囲を把握していないと、わからないことになる。
    #　3.ベイズ最適化
        #hyperopt；評価指標の指定。探索するパラメータの範囲の定義。探索回数の指定(100回ぐらいが十分とされている。)を指定して、パラメータ空間を探索する。

    #ポイントは、
        #重要なパラメータかどうか
        #モデルの複雑性を増すパラメータか、簡単にするパラメータか判断する。
        #乱数シードを指定して検討する。ランダム性なのか、それともパラメータの変化によるものなのかを検討するために。
    
    #GBDTのパラメータは、非常に色々ある。そのパラメータは、kaggle本のp.315あたりを参考にスべし。
    #NNのパラメータは、hp.choiceやhp.quniformを使いまくる。入力層と出力層はきまっているので、中間層・最適化関数しかパラメータはない。

#特徴選択および特徴量の重要度
    #単変量統計を用いる方法
        #相関係数->pandasのcorr関数がおすすめ、特徴選択に関しては、numpyのargsort関数がおすすめ。
        #カイ二乗統計量->統計量の大きいほうから特徴量を選択する方法。特徴量は非負の値で分類タスクである必要がある。スケールに影響されるので、
                        # minmaxscalerでスケーリングスべし。scikit-learnのfeature_selectionモジュールのchi2関数がおすすめ。
        #相互情報量->片方を知ることで、もう一方をより推測できるようになる場合に値が大きくなる。XとYが完全に従属のときには、どちらかの変数の情報量と等しくなる。独立なら0
                        #scikit-learnのfeature_selectionモジュールから、目的変数が連続変数ならmutual_info_regression関数。クラスならmutual_info_classif関数

    #特徴量の重要度を用いる方法
        #ランダムフォレストの特徴量の重要度->分岐を作成するときの基準となる値(回帰では、二乗誤差。分類では、ジニ不純度)
        #GBDTの特徴量の重要度->ゲイン・カバー・頻度

    #重要度を計算する方法。重要度を用いた応用的な特徴量選択の手法。特徴量の重要度を出力するライブラリを以下で述べる。
        #premutation importance->モデルを学習させた後に、通常通りに予測させた時にバリデーションデータのスコアと、ある特徴量の列をシャッフルしてその時のスコアを比較。
            #eli5というライブラリでおこなえる。
            #ランダムフォレストなら、rfpimpモジュールで計算できる。explained.aiというサイトでは重要度が考察されている。

        #null importance->目的変数をシャッフルして学習させた場合の重要度を基準にする。通常の重要度をactual importanceとする。
            #十分に予測力のある特徴量ならば、actual importance はnull importanceの100%より上になる。
        
        #boruta
            #それぞれの特徴量をシャッフルして、データを作成する。これを列方向に加えて、ランダムフォレストで学習を行い特徴量の重要度を計算する。
                #特徴量の高いモノのみを残して学修をおこなっていく。

        #特徴量を大量生成してから特徴量選択をする。

        #xgbfir
            #xgboostのモデルから決定木の分岐情報を抽出する。特徴量の重要度を出力するライブラリ。gainを基本に見れば良い。

    #反復して探索する手法
        #Greedy Forward Selection
            #特徴量を改善し、スコアを良くしていく。この改善が止まるまで続ける。
                #計算量は候補の特徴量の数に比例する。

#クラスの分布が偏っている場合
    #アンダーサンプリング(よく使われる)
        #負例のほうが多い場合に、負例の一部のみをもちいてモデルを学修させる方法。
        #特徴量を創るときは、負例のデータを使うのが望ましい。
        #すべてのデータを学習した場合とバリデーションの精度を比較し、精度の下がらないことを確認スべき
    #特に工夫をしない(よく使われる)
        #偏りのあるデータでも十分な精度が出せることがある。
    #重み付け
        #正例と負例のウェイト合計を等しくなるようにするもの。
    #オーバーサンプリング(あまり使われない)
        #負例が多すぎたら、正例を水増しする。SMOTEなどの人工的に正例を生成する方法がある。
    #確率を予測する必要がある場合の注意点
        #loglossなど適切な確率を予測する必要がある場合には、注意が必要で正例と負例の割合を変えたなら確率の補正をする必要がある。

#===================================================
# 6.アンサンブル
#===================================================

#シンプルなアンサンブル手法
    #平均、加重平均
        #モデルの精度を見ながら適当に決める。
        #スコアが最も高くなるように最適化する。
    #多数決
    #注意点とその他のテクニック
        #アンサンブルの後に最適化する。
        #不思議な調整
        #順位の平均を取る
        #幾何平均や調和平均・n乗しての平均を利用する。
        #過学習ぎみのモデルのアンサンブル。アンサンブルによってバリアンス(不安定さを抑えるもの)

#スタッキング
    #概要
        #1.学習データをクロスバリデーションのfoldにわける
        #2.out-of-foldで学習させ、バリデーションデータへの予測値を作成する。これによりそのモデルでの予測値という特徴量が生まれる。
        #3.各foldで学習させたモデルでのテストデータを予測し、平均などをとったものをテストデータの特徴量とする。
        #4.スタッキングしたい分だけ繰り返す。
        #5.予測と学修を行なう。

    #特徴量作成の方法としてのスタッキング
        #特徴量が増えて工夫のしがいのあるモデルが作れる。
    #スタッキングのポイント
        #データ量が多いコンベではかなり有効。データを食い尽くしてしまうのが問題。
        #テストデータの特徴量の作成方法
            #テストデータに対する予測を行う必要がある。
                #各foldのモデルにたいして平均をとる。
                #学習データ全体に対して学習し直したモデルで予測する。
    #hold-outデータへの予測値を用いたアンサンブル

#どんなモデルをアンサンブルするとよいか。
    #多様なモデルを使う
        #GBDT
        #NN
        #線形モデル
        #k近傍法
        #ランダムフォレスト
        #RGF
        #FFM
        #良いスタッキングのソリューションは、以下のモデルを含んで構成されることが多い。
            #2-3つのGBDT(決定木が浅いもの、中くらい、深いもの)
            #1-2つのランダムフォレスト(決定木が浅いもの、ふかいもの)
            #1-2つのNN(層がおおいもの、少ないもの)
            #1つの線形モデル
    #ハイパーパラメータを変える
        #交互作用の効き具合を変える
        #正則化の強さを変える
        #モデルの表現力を変える
    #特徴量を変える
        #特定の特徴量の組を使う/使わない
        #特徴量のスケーリングをする/しない
        #特徴選択を強く行なう/おこなわない
        #外れ値を除く/覗かない
        #データの前処理や返還の方法を変える
    #問題の捉え方を変える
        #問題を解くために新たに特徴量を生み出すことも検討する。
        #順番にモデルを作成していって、それからアンサンブルを考える。
    #スタッキングに含めるモデルの選択
        #相関係数が0.95かつ2つの母集団の確率分布が異なるかを検定するコルモゴロフ-スミルノフ検定統計量が0.05以上のモデルを精度が高い順に残す。
            #バリデーションの結果をログに出力して、各モデルのスコアを把握できるようにする
            #モデルの多様性を評価するために、モデルの予測値の相関関係を計算。異なるモデルの予測値同士の散布図をプロットする
            #モデルのバリデーションのスコアと、そのモデルの予測値を単独で提出した時のpublicスコアをプロットする。
    
#分析コンペでのアンサンブルの例